---
title: "Term Project 390.4- 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
Author: Juan D Astudillo, Vincent Miceli, Adriana Sham, Burhan Hanif, Sakib Salim
---

## R Markdown

```{r}
pacman::p_load(dplyr, tidyr, ggplot2, magrittr, stringr, mlr)
housing_data = read.csv("housing_data_2016_2017.csv")
```

##Delete variables that we dont need
```{r}
housing_data %<>%
  select(-c(HITId, HITTypeId, Title, Description, Keywords, Reward, CreationTime, MaxAssignments,	RequesterAnnotation,	AssignmentDurationInSeconds,	AutoApprovalDelayInSeconds,	Expiration,	NumberOfSimilarHITs, LifetimeInSeconds,	AssignmentId,	WorkerId,	AssignmentStatus,	AcceptTime,	SubmitTime,	AutoApprovalTime,	ApprovalTime,	RejectionTime,	RequesterFeedback,	WorkTimeInSeconds, LifetimeApprovalRate,	Last30DaysApprovalRate,	Last7DaysApprovalRate, URL, url, date_of_sale))
```

## Clean Data
```{r}
#extract zip codes as a separate feature
housing_data %<>%
  mutate( zip_code = str_extract(full_address_or_zip_code, "[0-9]{5}")) 

#make pets allowed binary and combine them
housing_data %<>%
  mutate(dogs_allowed = ifelse(substr(housing_data$dogs_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate(cats_allowed = ifelse(substr(housing_data$cats_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate( pets_allowed = ifelse( cats_allowed + dogs_allowed > 0, 1, 0)) %>%
  mutate(coop_condo = factor(tolower(coop_condo)))

housing_data %<>%
  select(-c(dogs_allowed,cats_allowed, fuel_type))

d = housing_data

#convert NA's to 0 for charges
d %<>%
  mutate(maintenance_cost = sjmisc::rec(maintenance_cost, rec = "NA = 0 ; else = copy")) %<>%
  mutate(common_charges = sjmisc::rec(common_charges, rec = "NA = 0 ; else = copy"))##recode from NA to 0.

# combine maintaince cost and common charges
d %<>% 
  mutate( monthly_cost = common_charges + maintenance_cost)

d %<>%
  mutate(monthly_cost = sjmisc::rec(monthly_cost, rec = "0 = NA ; else = copy"))

## Convert garage to binary
d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = "NA = 0 ; else = copy")) ##recode from NA to 0. 

d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = " eys = 1; UG = 1 ; Underground = 1; yes = 1 ; Yes = 1 ; else = copy")) ##recode from NA to 0.

d %<>%
  select(-c(maintenance_cost , common_charges, model_type))

#str(d)
```

##Change variable type 

```{r}
d %<>%
  mutate( dining_room_type = as.factor(dining_room_type)) %>%
  mutate(garage_exists = as.character(garage_exists)) %>%
  mutate(garage_exists = as.numeric(garage_exists)) %>%
  mutate( parking_charges = as.character(parking_charges)) %>%
  mutate( parking_charges = as.numeric(parking_charges)) %>%
  #sale to numeric for regression
  mutate(sale_price = as.character(sale_price)) %>%
  mutate(sale_price = as.numeric(sale_price)) %>%
  mutate(total_taxes = as.character(total_taxes)) %>%
  mutate(total_taxes = as.numeric(total_taxes)) %>%
  
  #new feature 'price/sq ft'
  mutate(price_persqft = listing_price_to_nearest_1000 / sq_footage)
  
```


#Added latitude and longitude features using ggmap

```{r}
#Already run and included in the data
#pacman::p_load(ggmap)
#d %<>%
#  mutate(lat = geocode(full_address_or_zip_code)$lat, lon = #geocode(full_address_or_zip_code)$lon )

#geocoordinates for relevant LIRR stations
lirr_coord = read.csv("all_lirr_geocoordinates.csv")


RAD_EARTH = 3958.8
degrees_to_radians = function(angle_degrees){
  for(i in 1:length(angle_degrees))
    angle_degrees[i] = angle_degrees[i]*pi/180
  return(angle_degrees)
}

compute_globe_distance = function(destination, origin){
  destination_rad = degrees_to_radians(destination)
  origin_rad = degrees_to_radians(origin)
  delta_lat = destination_rad[1] - origin_rad[1]
  delta_lon = destination_rad[2] - origin_rad[2]
  h = (sin(delta_lat/2))^2 + cos(origin_rad[1]) * cos(destination_rad[1]) * (sin(delta_lon/2))^2
  central_angle = 2 * asin(sqrt(h))
  return(RAD_EARTH * central_angle)
}

#find the closest LIRR station and compute distance
shortest_lirr_distance = function(all_lirr_coords, house_coords){
  shortest_dist = Inf
  for (i in 1: nrow(all_lirr_coords)){
    ith_lirr = c(all_lirr_coords$lat[i], all_lirr_coords$lon[i])
    new_dist = compute_globe_distance(ith_lirr, house_coords)
    if( new_dist < shortest_dist){
      shortest_dist = new_dist
    }
  }
  return(shortest_dist)
}

d %<>%
  rowwise() %>%
  mutate(shortest_dist = shortest_lirr_distance(lirr_coord, c(lat, lon)) )

#makes any other addresses redundant
d %<>%
  select(-c(zip_code, full_address_or_zip_code, listing_price_to_nearest_1000))
```

We are trying to predict `sale_price`. So let's section our dataset:

```{r}

####CREATE A COLUMN ID

d %<>%
  ungroup(d) %>%
  mutate(id = 1 : 2230)

real_y = data.frame(d$id, d$sale_price)

j = d %>%
  select(total_taxes)

d %<>%
  select(-c(total_taxes, sale_price))

d = cbind(j, d)

d[,1][d[, 1] < 1000] = NA ## number 1 is total taxes

real_d = subset(d, (!is.na(d[,2])))  ## sale price
fake_d = subset(d, (is.na(d[,2])))
```
#Split the data that has y into train and test sets

```{r}
train_indices = sample(1 : nrow(real_d), nrow(real_d)*4/5)
training_data = real_d[train_indices, ]
testing_data = real_d[-train_indices, ]

#testing_data %<>%
#  mutate(sale_price = NA)


X = rbind(training_data, testing_data, fake_d)


#table(X$total_taxes)

#str(X)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
# head(M)
# summary(M)

```

Some of these missing indicators are collinear because they share all the rows they are missing on. Let's filter those out:

```{r}
M = tbl_df(t(unique(t(M))))
```


Some featuers did not have missingness so let's remove them:

```{r}
M %<>% select_if(function(x){sum(x) > 0})
# head(M)
# dim(M)
# colSums(M)

```

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 26,000! observations) so we will sample 5 for X1 and  for each of the trees and then average. That will be good enough.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(172, ncol(X)))$ximp
```


```{r}
Ximp %<>%
  arrange(id)


Xnew = data.frame(cbind(Ximp, M, real_y))

Xnew %<>%
  mutate(price = d.sale_price) %>%
  select(-c(id, d.id, d.sale_price))
  

linear_mod_impute_and_missing_dummies = lm(price ~ ., data = Xnew)
summary(linear_mod_impute_and_missing_dummies)

```


### REMOVING MISSING Y SECTION
```{r}
Data = Xnew
### sale price is our imputed Y


Data %<>%
  filter(!is.na(price))


Y = Data$price

Xtrain = Data[1:422, ]
Xtest = Data[423:528, ]

Ytrain = Y[1:422]
Ytest = Y[423:528]

dtrain = cbind(Xtrain, Ytrain) ## combine x train with y train, x test with y test
dtest = cbind(Xtest, Ytest)
```

Linear Regression

```{r}
Xtrain$price = NULL
Xtest$price = NULL
linear = lm(Ytrain ~ ., data = Xtrain)## simple linear model
summary(linear)
```


```{r}
yhat = predict(linear, Xtest)

e = yhat - Ytest

#RMSE
sqrt(sum(e^2) / 108)
```

```{r}
#REGRESSION TREE
pacman::p_load(YARF)
reg_tree = YARFCART(Xtrain, Ytrain)
y_hat_test_tree = predict(reg_tree, Xtest)
e = Ytest - y_hat_test_tree
#RMSE
sqrt(sum(e^2)/108)
sd(e)
illustrate_trees(reg_tree, max_depth = 4, open_file = TRUE)
```


Make test, train and selection sets

```{r}
n = nrow(Data)
K = 5
test_indices = sample(1 : n, size = n * 1 / K)
master_train_indices = setdiff(1 : n, test_indices)
select_indices = sample(master_train_indices, size = n * 1 / K)
train_indices = setdiff(master_train_indices, select_indices)
rm(master_train_indices)

houses_train = Data[train_indices, ]
houses_select = Data[select_indices, ]
houses_test = Data[test_indices, ]

```


Hyperparameter Tuning for Random Forest
Running this chunk gives the optimal hyperparameters used in the model.

```{r}
# train_task = makeRegrTask(data = houses_train, target = "price")
# test_task = makeRegrTask(data = houses_test, target = "price")
# 
# algorithm = makeLearner("regr.randomForest", predict.type = "response")
# 
# all_mtry = seq(1, 10, by = 1)
# all_nodesize = seq(1, 10, by = 1)
# all_sampsize = seq(100, 110, by = 1)
# all_hyperparams = makeParamSet(
#   makeDiscreteParam(id = "nodesize", default = 5, values = all_nodesize),
#   makeDiscreteParam(id = "mtry", default = 5, values = all_mtry)
# )
# inner = makeResampleDesc("CV", iters = 3)
# lrn = makeTuneWrapper("regr.randomForest", 
#                       resampling = inner, 
#                       par.set = all_hyperparams, 
#                       control = makeTuneControlGrid())
# 
# 
# outer = makeResampleDesc("CV", iters = 5)
# r = resample(lrn, train_task, 
#             resampling = outer, 
#             extract = getTuneResult)
# 
# r #overall estimate of oos error of the whole procedure if it were used on all of $\mathbb{D}$
# print(getNestedTuneResultsOptPathDf(r)) #results of each inner validation over all outer iterations
# r$extract #"winning" model for each outer iteration

```

```{r}

rf_mod = YARF(Xtrain, Ytrain, mtry = 10, nodesize = 4)

y_hat_test_tree = predict(rf_mod, Xtest)
e = Ytest - y_hat_test_tree
#RMSE
sqrt(sum(e^2)/108)
sd(e)
```

