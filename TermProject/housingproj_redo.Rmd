---
title: "Term Project 390.4- 2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
Author: Juan D Astudillo, Vincent Miceli, Adriana Sham, Burhan Hanif, Sakib Salim
---

## Introduction

For this real-estate price prediction project, our team was given a dataset scraped from zillow.com that included features of apartment/condo listings such as the number of bedrooms, the square footage, the address, and some less relevant information.  We were tasked with cleaning this dataset, which had many missing values, and building the most accurate machine learning model possible from it.

```{r}
pacman::p_load(dplyr, tidyr, ggplot2, magrittr, stringr, mlr)
housing_data = read.csv("housing_data_2016_2017.csv")
```

###Delete features that are irrelevant to sale price

From a quick look we identified many features that were going to be irrelevant to the model so we removed them from the start.  Listing price was removed because we believed that would be an unfair feature if we're looking to build a model to value properties.  The model could be used to get an idea of a fair listing price for a property. 

```{r}
housing_data <- housing_data[29:57] %>%
  select(-c(date_of_sale, model_type, listing_price_to_nearest_1000, url))
```

## Cleaning the Data
In this block of code we cleaned the addresses of the listings, converted some features to binary variables, and performed other general cleaning of the dataset.


```{r}
housing_data %<>%
  mutate( zip_code = str_extract(full_address_or_zip_code, "[0-9]{5}")) %>%
  mutate(dogs_allowed = ifelse(substr(housing_data$dogs_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate(cats_allowed = ifelse(substr(housing_data$cats_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate(coop = as.integer(coop_condo == 'co-op')) %>%
  mutate(condo = as.integer(coop_condo == 'condo')) %>%
  select(-coop_condo)

d = housing_data
d %<>%
  mutate(maintenance_cost = sjmisc::rec(maintenance_cost, rec = "NA = 0 ; else = copy")) %<>%
  mutate(common_charges = sjmisc::rec(common_charges, rec = "NA = 0 ; else = copy"))##recode from NA to 0.
# combine maintaince cost and common charges
d %<>% 
  mutate( monthly_cost = common_charges + maintenance_cost)
d %<>%
  mutate(monthly_cost = sjmisc::rec(monthly_cost, rec = "0 = NA ; else = copy"))
## convert garage_exists feature to binary
d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = "NA = 0 ; else = copy")) ##recode from NA to 0. 
d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = " eys = 1; UG = 1 ; Underground = 1; yes = 1 ; Yes = 1 ; else = copy")) ##recode from NA to 0.
d %<>%
  select(-c(maintenance_cost , common_charges))
```

## Added latitude and longitude features using ggmap

We had the idea that latitudes and longitudes would be a better measure of location than zip codes or addresses, so we engineered those features using ggmap.  We could then remove the original location related features.  We also engineered a feature that calculates the distance from each coop/condo to the nearest LIRR station, as properties nearest to LIRR stations might be more desirable to those working in Manhattan.

```{r error = TRUE}
#Already run and included in the data
#pacman::p_load(ggmap)
#d %<>%
#  mutate(lat = geocode(full_address_or_zip_code)$lat, lon = #geocode(full_address_or_zip_code)$lon )
#geocoordinates for relevant LIRR stations
lirr_coord = read.csv("coord.csv")
RAD_EARTH = 3958.8
degrees_to_radians = function(angle_degrees){
  for(i in 1:length(angle_degrees))
    angle_degrees[i] = angle_degrees[i]*pi/180
  return(angle_degrees)
}
compute_globe_distance = function(destination, origin){
  destination_rad = degrees_to_radians(destination)
  origin_rad = degrees_to_radians(origin)
  delta_lat = destination_rad[1] - origin_rad[1]
  delta_lon = destination_rad[2] - origin_rad[2]
  h = (sin(delta_lat/2))^2 + cos(origin_rad[1]) * cos(destination_rad[1]) * (sin(delta_lon/2))^2
  central_angle = 2 * asin(sqrt(h))
  return(RAD_EARTH * central_angle)
}
#find the closest LIRR station and compute distance
shortest_lirr_distance = function(all_lirr_coords, house_coords){
  shortest_dist = Inf
  for (i in 1: nrow(all_lirr_coords)){
    ith_lirr = c(all_lirr_coords$lat[i], all_lirr_coords$lon[i])
    new_dist = compute_globe_distance(ith_lirr, house_coords)
    if( new_dist < shortest_dist){
      shortest_dist = new_dist
    }
  }
  return(shortest_dist)
}
d %<>%
  rowwise() %>%
  mutate(shortest_dist = shortest_lirr_distance(lirr_coord, c(lat, lon)) ) %>%
  select(-c(zip_code, full_address_or_zip_code, community_district_num))

str(d)
```

From an overview we identified features that needed the variable types to be changed

```{r}
d$garage_exists = as.character(d$garage_exists)
d$garage_exists = as.integer(d$garage_exists)
d$parking_charges = as.character(d$parking_charges) 
d$parking_charges = as.numeric(d$parking_charges)
d$sale_price = as.character(d$sale_price)
d$sale_price = as.numeric(d$sale_price)
d$total_taxes = as.character(d$total_taxes) 
d$total_taxes = as.numeric(d$total_taxes)


str(d)
```

## Exploratory Data Analysis

We now may be able to convert the entire dataset to numeric, so that it will perform better with regression models.  We will examine the remaining categorical variables as the first step in our exploratory data analysis.

```{r}
table(d$dining_room_type, useNA = 'always')
```

We can combine dining area, none, and the NAs into the other category.

```{r}
other <- d %>%
  filter(is.na(dining_room_type) | dining_room_type == 'none' | dining_room_type == 'dining area' | dining_room_type == 'other') %>%
  mutate(dining_room_type = "other")

non_na <- d %>%
  filter(dining_room_type == 'combo' | dining_room_type == 'formal')

d <- rbind(other, non_na)
d$dining_room_type <- as.factor(d$dining_room_type)
```

```{r}
table(d$fuel_type, useNA = 'always')
```

We can do a similar process for fuel type that we did for dining room type, compressing the number of categories.

```{r}
other <- d %>%
  filter(is.na(fuel_type) | fuel_type == 'none' | fuel_type == 'other' | fuel_type == 'Other') %>%
  mutate(fuel_type = "other")

non_na <- d %>%
  filter(fuel_type == 'electric' | fuel_type == 'gas' | fuel_type == 'oil')

d <- rbind(other, non_na)
d$fuel_type <- as.factor(d$fuel_type)
```

```{r}
table(d$kitchen_type, useNA = 'always')
```

We are going to assume a combo kitchen and an eat in kitchen are one in the same, and will go with the mode for missing values (combo)

```{r}
combo <- d %>%
  filter(is.na(kitchen_type) | kitchen_type == 'none' | kitchen_type == 'combo' | kitchen_type == 'eat in') %>%
  mutate(kitchen_type = 'combo')

efficiency <- d %>%
  filter(kitchen_type == 'efficiency')

d <- rbind(combo, efficiency)
d$kitchen_type <- as.factor(d$kitchen_type)
```

Now to convert the categorical variables to dummy variables, and combine them to the dataset:

```{r}
d <- cbind(model.matrix( ~ dining_room_type - 1, d), model.matrix( ~ fuel_type - 1, d), model.matrix( ~ kitchen_type - 1, d), d)

d %<>%
  select(-c(dining_room_type, fuel_type, kitchen_type))
```

Now we will take a look at the missingness of the remaining features

```{r}
cols <- colnames(d)
for (i in 1:length(d)) {
  cat(cols[i])
  cat('\n')
  cat(sum(is.na(d[i])))
  cat('\n')
}
```

A quick overview of the missingness in the dataset showed us a few things.  A majority of the sale_price feature, our target variable, was missing.  These rows can still be useful though for imputation of the other missing values.  There was a very high amount of missing values in the parking_charges, pct_tax_deductibl, and num_half_bathrooms, and we decided these features were not extremely important to the model, so we went ahead and removed those.  

```{r}
d %<>%
  select(-c(parking_charges, pct_tax_deductibl, num_half_bathrooms))
```

There is also a significant amount of missing values in the total taxes column, so we may be able to handle that manually as well.

```{r}
table(d$total_taxes)
```

Aside from the missing values, there are many values that are impossible/incorrect as they are much too low to make sense.  We decided to handle those manually by replacing them with the mean. 

```{r}
d %<>%
  mutate(total_taxes = ifelse(total_taxes < 1000, mean(d$total_taxes), total_taxes))
```


```{r}
str(d)
```

To handle the remaining missing values, we believe the best method is to use missForest to impute them, essentially predicting what the missing values probably would be or would be close to based on other features of the row. Before we do that though, we need to make sure we remember which rows did not have a value in sale price as those data points should not be used for the model.

```{r}
df <- d %>%
  mutate(sale_miss = ifelse(is.na(sale_price), 1, 0))
```


```{r}
pacman::p_load(missForest)

df_imp = missForest(df, ntree = 300,)$ximp
```

All the missing values have now been imputed, so we can now remove the section of the dataset where sale_price was missing.  

```{r}
df <- df_imp %>%
  filter(sale_miss == 0) %>%
  select(-sale_miss)
```

## Regression

```{r}
summary(lm(sale_price ~ ., df))
```

A simple linear model is able to predict the sale_price with an R-squared of .824. Let's see how this model performs out-of-sample.

```{r}
##Train-Test Split
pacman::p_load(caTools)
set.seed(77)
spl = sample.split(df$sale_price, 0.75)
train = subset(df, spl == TRUE)
test = subset(df, spl == FALSE)
```

```{r}
simple <- lm(sale_price ~ ., train)
yhat <- predict(simple, test)

rsq <- function (preds, actual) {
  rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
  tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
  r2 <- 1 - rss/tss
  r2
}

rsq(yhat, test$sale_price)
```

The simple linear model has an out-of-sample R-squared of .69, which is to be expected from a simple model from such a small dataset.  Let's see what we can do with forward step-wise regression to select the best predictors based on p-value.

```{r}
pacman::p_load(olsrr)
forward <- ols_step_forward_p(simple, penter = .05, details = FALSE, progress = TRUE)
```


```{r}
yhat <- predict(forward$model, test)
rsq(yhat, test$sale_price)
```

Using only the best 10 predictors, we had essentially the same R-squared from our model.  Let's see what we can do if we include these 10 predictors, as well as associations between them.

```{r}
top10 <- forward$predictors
f <- df %>%
  select(top10, sale_price)

set.seed(777)
spl = sample.split(f$sale_price, 0.75)
ftrain = subset(f, spl == TRUE)
ftest = subset(f, spl == FALSE)

associations <- lm(sale_price ~ . * ., ftrain)
forward <- ols_step_forward_p(associations, penter = .025, details = FALSE, progress = TRUE)
```

```{r}
yhat <- predict(forward$model, ftest)
rsq(yhat, ftest$sale_price)
```

That's a significant improvement, the out-of-sample R-squared jumped to .79 with this method. Just to check let's see how this method would work if we trained on the whole dataset (including the imputed sale_price points).

```{r}
f <- df_imp %>%
  select(top10, sale_price)

set.seed(77)
spl = sample.split(f$sale_price, 0.75)
ftrain = subset(f, spl == TRUE)
ftest = subset(f, spl == FALSE)

associations <- lm(sale_price ~ . * ., ftrain)
forward <- ols_step_forward_p(associations, penter = .005, details = FALSE, progress = TRUE)
```

```{r}
yhat <- predict(forward$model, test)
rsq(yhat, test$sale_price)
```

Even though this model was trained on essentially fake data, it still performed very well when predicting on the real data points.  This shows that this method at least has promise if we are able to scrape more data points that include the sale_price.

##RANDOM FOREST

Let's see how a random forest regression model performs with our real dataset. 

```{r}
pacman::p_load(randomForest)
rf <- randomForest(sale_price ~ ., train, mtry=3, ntree = 500, importance=TRUE)


yhat <- predict(rf, test)
rsq(yhat, test$sale_price)
```

The random forest regressor model had an R-squared of .89 out-of-sample, a huge improvement over the simple linear model.  This is also a very acceptable R-squared in general, considering we only had 528 data points.  Let's take a look at how a sample tree functioned in our random forest:

```{r}
pacman::p_load(rpart, rpart.plot)
tree <- rpart(
    formula = sale_price ~ .,
    data    = df,
    method  = "anova"
)

rpart.plot(tree)
```

What if we trained the random forest on the larger dataset:

```{r}
rf <- randomForest(sale_price ~ ., ftrain, ntry = 3, ntree = 500, importance = TRUE)

yhat <- predict(rf, test)
rsq(yhat, test$sale_price)
```

## Conclusion

An amazing .96 R-squared using the larger dataset to train the model, then predicting on the real data points.  There is definitely some over fitting going on when we train on the larger dataset though, because those missing sale prices were generated by the rest of the dataset's information.  It would definitely be interesting though to try these models on a larger, real dataset.