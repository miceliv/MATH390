---
Author: "Juan D Astudillo" 
title: "Term Project 390.4- 2019"
output: html_document
---

## R Markdown

```{r}
pacman::p_load(dplyr, tidyr, ggplot2, magrittr, stringr, mlr)
housing_data = read.csv("housing_data_2016_2017.csv")
```

##Delete variables that we dont need
```{r}
housing_data %<>%
  select(-c(HITId, HITTypeId, Title, Description, Keywords, Reward, CreationTime, MaxAssignments,	RequesterAnnotation,	AssignmentDurationInSeconds,	AutoApprovalDelayInSeconds,	Expiration,	NumberOfSimilarHITs, LifetimeInSeconds,	AssignmentId,	WorkerId,	AssignmentStatus,	AcceptTime,	SubmitTime,	AutoApprovalTime,	ApprovalTime,	RejectionTime,	RequesterFeedback,	WorkTimeInSeconds, LifetimeApprovalRate,	Last30DaysApprovalRate,	Last7DaysApprovalRate, URL, url, date_of_sale))
```
## Clean Data
```{r}
housing_data %<>%
  mutate( zip_code = str_extract(full_address_or_zip_code, "[0-9]{5}")) 

housing_data %<>%
  mutate(dogs_allowed = ifelse(substr(housing_data$dogs_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate(cats_allowed = ifelse(substr(housing_data$cats_allowed, 1, 3) == "yes", 1, 0)) %>%
  mutate( pets_allowed = ifelse( cats_allowed + dogs_allowed > 0, 1, 0)) %>%
  mutate(coop_condo = factor(tolower(coop_condo)))

housing_data %<>%
  select(-c(dogs_allowed,cats_allowed, fuel_type))

d = housing_data

d %<>%
  mutate(maintenance_cost = sjmisc::rec(maintenance_cost, rec = "NA = 0 ; else = copy")) %<>%
  mutate(common_charges = sjmisc::rec(common_charges, rec = "NA = 0 ; else = copy"))##recode from NA to 0.


# combine maintaince cost and common charges
d %<>% 
  mutate( monthly_cost = common_charges + maintenance_cost)

d %<>%
  mutate(monthly_cost = sjmisc::rec(monthly_cost, rec = "0 = NA ; else = copy"))

## Garage exists conver it to binary

d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = "NA = 0 ; else = copy")) ##recode from NA to 0. 

d %<>%
  mutate(garage_exists = sjmisc::rec(garage_exists, rec = " eys = 1; UG = 1 ; Underground = 1; yes = 1 ; Yes = 1 ; else = copy")) ##recode from NA to 0.

d %<>%
  select(-c(maintenance_cost , common_charges, model_type))


str(d)

```

##Change variable type 
```{r}
d %<>%
  mutate( dining_room_type = as.factor(dining_room_type)) %>%
  mutate(garage_exists = as.character(garage_exists)) %>%
  mutate(garage_exists = as.numeric(garage_exists)) %>%
  mutate( parking_charges = as.character(parking_charges)) %>%
  mutate( parking_charges = as.numeric(parking_charges)) %>%
  mutate(sale_price = as.character(sale_price)) %>%
  mutate(sale_price = as.numeric(sale_price)) %>%
  mutate(total_taxes = as.character(total_taxes)) %>%
  mutate(total_taxes = as.numeric(total_taxes)) %>%
  mutate(price_persqft = listing_price_to_nearest_1000 / sq_footage)
  
```


#Added latitude and longitude features using ggmap

```{r}
#pacman::p_load(ggmap)
#register_google(key = 'AIzaSyAA9F9nKHHRtmc2shoe4OHx24rFS4ZTjDA')
#d %<>%
#  mutate(lat = geocode(full_address_or_zip_code)$lat, lon = #geocode(full_address_or_zip_code)$lon )
d %<>%
  select(-c(zip_code, full_address_or_zip_code, listing_price_to_nearest_1000))


  
```

We are trying to predict `sale_price`. So let's section our dataset:

```{r}

####CREATE A COLUMN ID

d %<>%
  mutate(id = 1 : 2230)

real_y = data.frame(d$id, d$sale_price)

j = d %>%
  select(total_taxes)

d %<>%
  select(-c(total_taxes, sale_price))

d = cbind(j, d)

d

d[,1][d[, 1] < 1000] = NA ## number 1 is total taxes



  



real_d = subset(d, (!is.na(d[,2])))  ## sale price
fake_d = subset(d, (is.na(d[,2])))
```
#Split the data that has y into train and test sets

```{r}
train_indices = sample(1 : nrow(real_d), nrow(real_d)*4/5)
training_data = real_d[train_indices, ]
testing_data = real_d[-train_indices, ]

#testing_data %<>%
#  mutate(sale_price = NA)


X = rbind(training_data, testing_data, fake_d)


table(X$total_taxes)

str(X)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = tbl_df(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
head(M)
summary(M)

```

Some of these missing indicators are collinear because they share all the rows they are missing on. Let's filter those out:

```{r}
M = tbl_df(t(unique(t(M))))
```


Some featuers did not have missingness so let's remove them:

```{r}
M %<>% select_if(function(x){sum(x) > 0})
head(M)
dim(M)
colSums(M)

```

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 26,000! observations) so we will sample 5 for X1 and  for each of the trees and then average. That will be good enough.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(172, ncol(X)))$ximp
```


```{r}
Ximp %<>%
  arrange(id)

Ximp
real_y

Xnew = data.frame(cbind(Ximp, M, real_y))

Xnew

Xnew %<>%
  mutate(price = d.sale_price) %>%
  select(-c(id, d.id, d.sale_price))
  

Xnew
linear_mod_impute_and_missing_dummies = lm(price ~ ., data = Xnew)
summary(linear_mod_impute_and_missing_dummies)

```


### REMOVING MISSING Y SECTION
```{r}
Data = Xnew
### sale price is our imputed Y


Data %<>%
  filter(!is.na(price))


Y = Data$price

Xtrain = Data[1:422, ]
Xtest = Data[423:528, ]

Ytrain = Y[1:422]
Ytest = Y[423:528]

dtrain = cbind(Xtrain, Ytrain) ## combine x train with y train, x test with y test
dtest = cbind(Xtest, Ytest)
```

Linear Regression

```{r}
linear = lm(price ~ ., data = Data)## simple linear model
summary(linear)
```


```{r}
yhat = predict(linear, Xtest)

e = yhat - Ytest

sqrt(sum(e^2) / 108)

```

```{r}
#REGRESSION TREE

pacman::p_load(rsample)#data spliting
pacman::p_load(rpart) #performing reg tree
pacman::p_load(rpart.plot) #ploting reg tree
pacman::p_load(ipred) #bagging
pacman::p_load(caret) #bagging

Data$listing_price_to_nearest_1000 = NULL
Data$is_missing_listing_price_to_nearest_1000 = NULL

set.seed(1989)
final_split1 <- initial_split(Data, prop = .7)
final_train1 <- training(final_split1)
final_test1  <- testing(final_split1)

m1 = rpart(
  formula = price ~ .,
  data    = final_train1,
  method  = "anova"
  )

rpart.plot(m1)
plotcp(m1)

summary(m1)

```
 
```{r}
m2 <- rpart(
    formula = price ~ .,
    data    = final_train1,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(m2)

```

 
```{r}
m1$cptable


```
###Tunig
```{r}
m3 <- rpart(
    formula = price ~ .,
    data    = final_train1,
    method  = "anova", 
    control = list(minsplit = 10, maxdepth = 12, xval = 10)
)

m3$cptable

```
 
```{r}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)

head(hyper_grid)
nrow(hyper_grid)

```

```{r}
models <- list()

for (i in 1:nrow(hyper_grid)) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]

  # train a model and store in the list
  models[[i]] <- rpart(
    formula = price ~ .,
    data    = final_train1,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
    )
}
```

```{r}
# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}

hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
    ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)

```

```{r}
optimal_tree <- rpart(
    formula = price ~ .,
    data    = final_train1,
    method  = "anova",
    control = list(minsplit = 11, maxdepth = 8, cp = 0.01)
    )

pred <- predict(optimal_tree, newdata = final_test1)
RMSE(pred = pred, obs = final_test1$price)

```


##RANDOM FORESTS
```{r}
library(rsample)      # data splitting 
library(randomForest) # basic implementation
pacman::p_load(ranger)       # a faster implementation of randomForest
pacman::p_load(caret)        # an aggregator package for performing many machine learning models
pacman::p_load(h2o)          # an extremely fast java-based platform

set.seed(1989)
final_split1 <- initial_split(Data, prop = .7)
final_train1 <- training(final_split1)
final_test1  <- testing(final_split1)

```

```{r}

m1 <- randomForest(
  formula = price ~ .,
  data    = final_train1
)

m1

which.min(m1$mse)
## [1] 344

# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])

```

```{r}
# create training and validation data 
set.seed(1989)
valid_split <- initial_split(final_train1, .8)

# training data
final_train1_v2 <- analysis(valid_split)

# validation data
final1_valid <- assessment(valid_split)
x_test <- final1_valid[setdiff(names(final1_valid), "price")]
y_test <- final1_valid$price

rf_oob_comp <- randomForest(
  formula = price ~ .,
  data    = final_train1_v2,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")


```

```{r}
features <- setdiff(names(final_train1), "price")

set.seed(1989)

m2 <- tuneRF(
  x          = final_train1[features],
  y          = final_train1$price,
  ntreeTry   = 500,
  mtryStart  = 5,
  stepFactor = 1.5,
  improve    = 0.01,
  trace      = FALSE      # to not show real-time progress 
)


```

```{r}




# randomForest speed
system.time(
  ames_randomForest <- randomForest(
    formula = price ~ ., 
    data    = final_train1, 
    ntree   = 500,
    mtry    = floor(length(features) / 3)
  )
)
##    user  system elapsed 
##  55.371   0.590  57.364

# ranger speed
system.time(
  ames_ranger <- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3)
  )
)


```
